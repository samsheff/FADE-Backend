# Server Configuration
NODE_ENV=development
PORT=4000
HOST=0.0.0.0

# Database
DATABASE_URL="postgresql://terminal:terminal@postgres:5432/polymarket_terminal?schema=public"
REDIS_URL="redis://redis:6379"

# If running the backend outside Docker, switch DATABASE_URL/REDIS_URL to localhost:
# DATABASE_URL="postgresql://terminal:terminal@localhost:5432/polymarket_terminal?schema=public"
# REDIS_URL="redis://localhost:6379"

# Blockchain Configuration
POLYGON_RPC_URL=https://polygon-rpc.com
POLYGON_CHAIN_ID=137

# Compatibility aliases (not used by the app code directly)
# RPC_URL -> POLYGON_RPC_URL
# GAMMA_API_BASE -> POLYMARKET_GAMMA_API_URL
# CLOB_* -> POLYMARKET_CLOB_API_*
RPC_URL=
GAMMA_API_BASE=
CLOB_API_KEY=
CLOB_API_SECRET=
CLOB_API_PASSPHRASE=

# Polymarket CLOB API
POLYMARKET_CLOB_API_URL=https://clob.polymarket.com
POLYMARKET_GAMMA_API_URL=https://gamma-api.polymarket.com
POLYMARKET_DATA_API_URL=https://data-api.polymarket.com
# Minimum milliseconds between Gamma API requests (increase if rate limited)
GAMMA_API_REQUEST_INTERVAL_MS=2000
POLYMARKET_CLOB_API_KEY=
POLYMARKET_CLOB_API_SECRET=
POLYMARKET_CLOB_API_PASSPHRASE=
POLYMARKET_CLOB_SIGNER_ADDRESS=

# Polymarket Indexer
POLYMARKET_NETWORK=mainnet
POLYMARKET_RPC_URL=
POLYMARKET_MARKET_REGISTRY_ADDRESS=
POLYMARKET_MARKET_STATE_ADDRESS=

# Optional (reserved for future signer workflow)
SIGNER_PRIVATE_KEY=

# Contract Addresses (Polygon Mainnet)
CTF_EXCHANGE_ADDRESS=0x4bFb41d5B3570DeFd03C39a9A4D8dE6Bd8B8982E
CONDITIONAL_TOKENS_ADDRESS=0x4D97DCd97eC945f40cF65F87097ACe5EA0476045
USDC_ADDRESS=0x2791Bca1f2de4661ED88A30C99A7a9449Aa84174

# Rate Limiting
RATE_LIMIT_MAX=100
RATE_LIMIT_WINDOW=60000

# Security
NONCE_TTL_MS=300000

# Caching
MARKET_CACHE_TTL_MS=60000
ORDERBOOK_CACHE_TTL_MS=30000
ORDERBOOK_SNAPSHOT_TTL_MS=30000

# WebSocket Configuration
CLOB_WS_HEARTBEAT_MS=15000
CLOB_WS_RECONNECT_BASE_MS=1000
CLOB_WS_RECONNECT_MAX_MS=30000

# Background Jobs
MARKET_SYNC_INTERVAL_MS=60000
POSITION_UPDATE_INTERVAL_MS=300000

# Historical Data Backfill
# Delay between Data API requests (milliseconds) - increase if rate limited
HISTORICAL_BACKFILL_RATE_LIMIT_MS=1000
# Number of trades to fetch per API request (max 10,000)
HISTORICAL_BACKFILL_BATCH_SIZE=5000

# Market Data Management
# Automatically mark markets as inactive when CLOB API returns 404 for orderbooks
# Set to 'true' to enable, 'false' or omit to disable (default: false)
# Recommended: Start with 'false' to observe behavior, then enable after verification
AUTO_DEACTIVATE_CLOSED_MARKETS=false

# ============================================================================
# EDGAR Worker Configuration
# ============================================================================

# Enable/disable EDGAR filing worker
EDGAR_WORKER_ENABLED=false

# Sync interval (milliseconds) - how often to run the full pipeline
EDGAR_SYNC_INTERVAL_MS=3600000

# Universe discovery sync interval (milliseconds) - how often to sync full SEC issuer list
# Default: 86400000 (24 hours)
EDGAR_UNIVERSE_SYNC_INTERVAL_MS=86400000

# Local storage path for downloaded filings
# Docker: This path is mounted to a persistent volume (terminal_edgar_storage)
# Local dev: Use ./storage/edgar (relative to back/ directory)
EDGAR_STORAGE_PATH=./storage/edgar

# SEC API user agent (required by SEC - must include contact email)
EDGAR_API_USER_AGENT=Trading Terminal Bot (contact@yourcompany.com)

# Rate limit between SEC API requests (milliseconds) - SEC allows 10 req/sec max
EDGAR_API_RATE_LIMIT_MS=100

# Number of filings to process per batch in each pipeline stage
EDGAR_BATCH_SIZE=10

# Discovery mode - automatically discover new filings matching toxic financing patterns
EDGAR_DISCOVERY_MODE=true

# Historical Backfill Configuration
# Enable one-time historical backfill on startup (uses SEC Historical Search API)
EDGAR_BACKFILL_ENABLED=true

# Lookback window for historical backfill (days)
# Default: 90 days (recommended for initial backfill)
EDGAR_BACKFILL_LOOKBACK_DAYS=90

# Maximum pages to fetch per form type during backfill (100 filings per page)
# Default: 50 pages = 5,000 filings max per form type
# Increase if you need deeper historical coverage
EDGAR_BACKFILL_MAX_PAGES_PER_FORM=50

# Signal computation thresholds
SIGNAL_DILUTION_SHELF_THRESHOLD_PCT=20
SIGNAL_TOXIC_PRICE_THRESHOLD=2
SIGNAL_REVERSE_SPLIT_LOOKBACK_MONTHS=12

# ============================================================================
# News Worker Configuration
# ============================================================================

# Enable/disable News worker
NEWS_WORKER_ENABLED=false

# Sync interval (milliseconds) - how often to poll for new articles
# Default: 3600000 (1 hour)
NEWS_SYNC_INTERVAL_MS=3600000

# Local storage path for downloaded articles
# Docker: This path is mounted to a persistent volume (terminal_news_storage)
# Local dev: Use ./storage/news (relative to back/ directory)
NEWS_STORAGE_PATH=./storage/news

# Number of articles to process per batch in each pipeline stage
NEWS_BATCH_SIZE=20

# Historical Backfill Configuration
# Enable one-time historical backfill on startup (fetches articles from lookback window)
NEWS_BACKFILL_ENABLED=true

# Lookback window for historical backfill (days)
# Default: 7 days (Finnhub free tier limitation)
NEWS_BACKFILL_LOOKBACK_DAYS=7

# Finnhub API Configuration
# Get your free API key at: https://finnhub.io/register
FINNHUB_API_KEY=

# Finnhub API base URL
FINNHUB_API_BASE_URL=https://finnhub.io

# Rate limit between Finnhub API requests (milliseconds)
# Free tier: 60 req/min = 1 req/sec
# Default: 1000ms (1 req/sec)
FINNHUB_API_RATE_LIMIT_MS=1000

# Signal extraction threshold (0-1)
# News signals with confidence below this threshold will be filtered out
# Default: 0.7
SIGNAL_NEWS_MIN_CONFIDENCE=0.7

# ============================================================================
# Entity Enrichment Configuration
# ============================================================================

# Enable/disable entity enrichment job (classification, competitors, factor exposures)
ENTITY_ENRICHMENT_ENABLED=true

# Enrichment interval (milliseconds) - how often to refresh stale classifications
# Default: 604800000 (7 days / weekly)
ENTITY_ENRICHMENT_INTERVAL_MS=604800000

# Number of instruments to process per batch
ENTITY_ENRICHMENT_BATCH_SIZE=100

# ============================================================================
# Signal Computation Configuration
# ============================================================================

# Enable/disable signal computation job (competitor impact, factor correlation, cross-entity propagation)
SIGNAL_COMPUTATION_ENABLED=true

# Computation interval (milliseconds) - how often to generate new signals
# Default: 900000 (15 minutes)
SIGNAL_COMPUTATION_INTERVAL_MS=900000

# Minimum confidence threshold for signals (0-1)
# Signals below this threshold will be filtered out
# Default: 0.6
SIGNAL_MIN_CONFIDENCE_THRESHOLD=0.6

# Signal expiration period (days)
# Signals older than this will be automatically deleted
# Default: 7 days
SIGNAL_EXPIRATION_DAYS=7

# ============================================================================
# Elasticsearch Configuration
# ============================================================================

ELASTICSEARCH_URL=http://localhost:9200
ELASTICSEARCH_INDEX_PREFIX=terminal_
SEARCH_INDEXER_ENABLED=true
SEARCH_INDEXER_BATCH_SIZE=100

# Logging
# Options: fatal, error, warn, info, debug, trace
# Use 'debug' to see detailed EDGAR API activity
LOG_LEVEL=info
